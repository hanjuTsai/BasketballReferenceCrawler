{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import requests as rq\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from selenium import webdriver\n",
    "from IPython.display import clear_output\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreigner(html):\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('p')\n",
    "    # print(links)\n",
    "    # <span itemprop=\"birthPlace\">\n",
    "\n",
    "    links = soup.find_all(['span'] , attrs = { 'itemprop' : 'birthPlace'})\n",
    "    for link in links:\n",
    "        for c in (link.descendants):\n",
    "            aa = ((c.find_next('a')).attrs)\n",
    "            if 'country=US' in aa['href']:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should exccute the chromedriver and get the page source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def league(html):\n",
    "    result = []\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('div', attrs = {'class': 'leaderboard_wrapper' , 'id': 'all_leaderboard'})\n",
    "\n",
    "    comments = soup.findAll(text=lambda text:isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        a = comment.find('div_leaderboard')\n",
    "        if (a != -1):\n",
    "            #print(a,comment)\n",
    "            break\n",
    "            \n",
    "    ts = BeautifulSoup(comment.string, 'html.parser')\n",
    "    ts = ts.find_all('div', {'id' : 'leaderboard_all_league' , 'class' : 'data_grid_box'})\n",
    "    for tt in ts:\n",
    "        kk = tt.table.find_all('a')\n",
    "        for k in kk:\n",
    "            result.append((k.text))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should exceute the chromedriver and get the page source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allstar(html):\n",
    "    result = []\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('div', attrs = {'class': 'leaderboard_wrapper' , 'id': 'all_leaderboard'})\n",
    "\n",
    "    comments = soup.findAll(text=lambda text:isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        a = comment.find('div_leaderboard')\n",
    "        if (a != -1):\n",
    "            #print(a,comment)\n",
    "            break\n",
    "    ts = BeautifulSoup(comment.string, 'html.parser')\n",
    "    ts = ts.find_all('div', {'id' : 'leaderboard_allstar' , 'class' : 'data_grid_box'})\n",
    "    for tt in ts:\n",
    "        kk = tt.table.find_all('a')\n",
    "        for k in kk:\n",
    "            result.append((k.text))\n",
    "            \n",
    "    result = list(map (lambda x : str(int(x.split()[0])-1 ) + '-' + (x.split()[0][2:])  , result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hall(html):\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('li' , {'class'  : 'bling_special bling_hof'})\n",
    "    for link in links:\n",
    "        if link.text == \"Hall of Fame\":\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def btn_click(wd,url):\n",
    "    wd.get(url)\n",
    "    script = \"vjs_addClass(document.getElementById('leaderboard_allstar'),'show_all')\"\n",
    "    script2 = \"vjs_addClass(document.getElementById('leaderboard_allleague'),'show_all')\"\n",
    "    btn = wd.execute_script(script)\n",
    "    btn = wd.execute_script(script2)\n",
    "    return(wd.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_info(wd,url):\n",
    "    html = btn_click(wd,url)\n",
    "    result = ([Hall(html) ,league(html), allstar(html), foreigner(html)])\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "df = pd.read_csv('./info/player_my_df.csv')\n",
    "info = pd.read_excel('./info/info.xlsx')\n",
    "\n",
    "dirs = !ls ./data\n",
    "dirs_no = sorted(list(map (lambda x : int(x.split('.')[0]),dirs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_game_table(html):\n",
    "    \n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')    \n",
    "    links = soup.find_all('table', { 'id' : 'per_game'} )\n",
    "    \n",
    "    if (len(links) == 0):\n",
    "        return\n",
    "\n",
    "    rows = links[0].find_all('tr')\n",
    "\n",
    "    colname = []\n",
    "    rowname = []\n",
    "    for header in links[0].find_all('th', {'scope' : 'col'}):\n",
    "        colname.append(header.text)\n",
    "        \n",
    "    for header in links[0].find_all('th', {'scope' : 'row'}):\n",
    "        rowname.append(header.text)\n",
    "        \n",
    "    table =[]\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    rnum = 0\n",
    "    for row in links[0].find_all('tr'):\n",
    "        table = []\n",
    "        columns = row.find_all('td')\n",
    "        \n",
    "        for column in columns:\n",
    "            ex = column.text.split('-')\n",
    "            try:\n",
    "                if len(ex) > 1 and int(ex[0]):\n",
    "                    rowname.insert(rnum, column.text)\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            table.append(column.text)\n",
    "            \n",
    "        if not table :\n",
    "            continue   \n",
    "            \n",
    "        rnum = rnum + 1    \n",
    "        df = df.append(pd.Series(table),ignore_index=True)\n",
    "    \n",
    "    df[colname[0]] = pd.Series(rowname)\n",
    "    df.columns = colname[-len(colname) + 1 :] + colname[:-len(colname) + 1 ]\n",
    "    return(df[colname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the all data processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "81\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "dirs = !ls ./data\n",
    "dirs_no = sorted(list(map (lambda x : int(x.split('.')[0]),dirs)))\n",
    "\n",
    "with open ('./info/all_data', 'rb') as fp:\n",
    "    data = pickle.load(fp)\n",
    "      \n",
    "info = pd.read_excel('./info/info2.xlsx')\n",
    "\n",
    "col = ['Season', 'Age', 'Tm', 'Lg', 'Pos', 'G', 'GS', 'MP', 'FG', 'FGA', 'FG%',\n",
    "       '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%',\n",
    "       'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS']\n",
    "\n",
    "url = pd.read_csv('./info/player_my_df.csv')['5']\n",
    "df = pd.DataFrame(columns=col)   \n",
    "row = len(data)\n",
    "ind = info.columns\n",
    "\n",
    "for i in range(row):\n",
    "    if(len(dirs_no) > 0):\n",
    "        if i != dirs_no[0]:\n",
    "            continue\n",
    "        dirs_no.pop(0)\n",
    "    \n",
    "    response = rq.get(url[i])\n",
    "    html = response.text\n",
    "    per_game_table(html) \n",
    "    my_list = per_game_table(html)\n",
    "    \n",
    "    \n",
    "    pp = list(my_list['Season']).index('Career')\n",
    "    my_list = my_list.loc[:pp,:]\n",
    "\n",
    "    r , c = ((my_list.shape))\n",
    "    \n",
    "    small_df = my_list\n",
    "\n",
    "\n",
    "    for j in range(len(col)):\n",
    "        if col[j] not in small_df.columns:\n",
    "            small_df[col[j]] = pd.Series([' ']* r)\n",
    "    \n",
    "    \n",
    "    ## Process fix award ##\n",
    "    ## [None, ['1988-89'], ['1989-90'], False]\n",
    "    award = data[i]\n",
    "    inf = list(info.iloc[i,:])\n",
    " \n",
    "    if(award[0]):\n",
    "        small_df['名人堂']= pd.Series([1]*r)\n",
    "    else:\n",
    "        small_df['名人堂']= pd.Series([0]*r)\n",
    "\n",
    "    if(award[3]):\n",
    "        small_df['外籍球員']= pd.Series([1]*r)\n",
    "    else:\n",
    "        small_df['外籍球員']= pd.Series([0]*r)\n",
    "\n",
    "    small_df['是否入圍全明星'] = pd.Series([0]*r)\n",
    "    small_df['當年度獎項次數'] = pd.Series([0]*r)\n",
    "    small_df['當年度獎項YN'] = pd.Series([0]*r)\n",
    "    \n",
    "    \n",
    "    #([Hall(html) ,league(html), allstar(html), foreigner(html)])\n",
    "    \n",
    "    all_series = list(small_df.iloc[:,0])\n",
    "    \n",
    "    for m in range(len(award[2])):\n",
    "        for n in range(len(all_series)):\n",
    "            if(award[2][m] == all_series[n]):\n",
    "                small_df['是否入圍全明星'][n] = 1\n",
    "                \n",
    "    small_df['當年度獎項次數'][r-1] = sum(list(small_df['當年度獎項次數']))\n",
    "    \n",
    "    for m in range(len(award[1])):\n",
    "        for n in range(len(all_series)):\n",
    "            if(award[1][m] == all_series[n]):\n",
    "                small_df['當年度獎項次數'][n] += 1\n",
    "                small_df['當年度獎項YN'][n] = 1\n",
    "\n",
    "    ## Process info ##\n",
    "    for k in range(len(inf)):\n",
    "        small_df[ind[k]]= pd.Series([inf[k]] * r)\n",
    "   \n",
    "    year_play = pd.Series([0]*r)  \n",
    "    all_series.pop(-1)    \n",
    "    compact_id = list(set(all_series))\n",
    "    compact_id = sorted(compact_id,key= lambda x : int(x.split('-')[0]))\n",
    "   \n",
    "    for j in range(len(all_series)):\n",
    "        front = list(small_df['Tm'])[:j]\n",
    "        count = 0\n",
    "        for k in range(len(front)):\n",
    "            if 'Did' in front[k]:\n",
    "                count += 1\n",
    "        year_play[j] = compact_id.index(all_series[j]) - count + 1 \n",
    "        \n",
    "    year_play[j+1] = list(year_play)[-2]\n",
    "    \n",
    "    small_df['年資'] = year_play\n",
    "    \n",
    "    df = pd.concat([small_df,df],axis=0,join='outer',ignore_index=True,sort=False)\n",
    "    \n",
    "    if i % 20 == 1:\n",
    "        print(i)\n",
    "\n",
    "    if i % 200 == 1:\n",
    "        clear_output()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "cols = cols[-15:] + cols[:-15]\n",
    "cols = ['FirstName', 'LastName', 'From', 'To',\\\n",
    "        '位置', '呎', '吋', '磅', 'Colleges','出生年', '出生月', \\\n",
    "        '出生日', '選秀順位','名人堂' , '當年度獎項次數', '當年度獎項YN' , '是否入圍全明星','外籍球員', '年資', \\\n",
    "        'Season', 'Age', 'Tm', 'Lg', 'Pos', 'G', 'GS', 'MP',\\\n",
    "        'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%',\\\n",
    "        'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = df[cols]\n",
    "ss.to_excel('Final7.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
