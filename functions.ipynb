{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import requests as rq\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from selenium import webdriver\n",
    "from IPython.display import clear_output\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreigner(html):\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('p')\n",
    "    # print(links)\n",
    "    # <span itemprop=\"birthPlace\">\n",
    "\n",
    "    links = soup.find_all(['span'] , attrs = { 'itemprop' : 'birthPlace'})\n",
    "    for link in links:\n",
    "        for c in (link.descendants):\n",
    "            aa = ((c.find_next('a')).attrs)\n",
    "            if 'country=US' in aa['href']:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should exccute the chromedriver and get the page source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def league(html):\n",
    "    result = []\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('div', attrs = {'class': 'leaderboard_wrapper' , 'id': 'all_leaderboard'})\n",
    "\n",
    "    comments = soup.findAll(text=lambda text:isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        a = comment.find('div_leaderboard')\n",
    "        if (a != -1):\n",
    "            #print(a,comment)\n",
    "            break\n",
    "            \n",
    "    ts = BeautifulSoup(comment.string, 'html.parser')\n",
    "    ts = ts.find_all('div', {'id' : 'leaderboard_all_league' , 'class' : 'data_grid_box'})\n",
    "    for tt in ts:\n",
    "        kk = tt.table.find_all('a')\n",
    "        for k in kk:\n",
    "            result.append((k.text))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should exceute the chromedriver and get the page source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allstar(html):\n",
    "    result = []\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('div', attrs = {'class': 'leaderboard_wrapper' , 'id': 'all_leaderboard'})\n",
    "\n",
    "    comments = soup.findAll(text=lambda text:isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        a = comment.find('div_leaderboard')\n",
    "        if (a != -1):\n",
    "            #print(a,comment)\n",
    "            break\n",
    "    ts = BeautifulSoup(comment.string, 'html.parser')\n",
    "    ts = ts.find_all('div', {'id' : 'leaderboard_allstar' , 'class' : 'data_grid_box'})\n",
    "    for tt in ts:\n",
    "        kk = tt.table.find_all('a')\n",
    "        for k in kk:\n",
    "            result.append((k.text))\n",
    "            \n",
    "    result = list(map (lambda x : str(int(x.split()[0])-1 ) + '-' + (x.split()[0][2:])  , result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hall(html):\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('li' , {'class'  : 'bling_special bling_hof'})\n",
    "    for link in links:\n",
    "        if link.text == \"Hall of Fame\":\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def btn_click(wd,url):\n",
    "    wd.get(url)\n",
    "    script = \"vjs_addClass(document.getElementById('leaderboard_allstar'),'show_all')\"\n",
    "    script2 = \"vjs_addClass(document.getElementById('leaderboard_allleague'),'show_all')\"\n",
    "    btn = wd.execute_script(script)\n",
    "    btn = wd.execute_script(script2)\n",
    "    return(wd.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_info(wd,url):\n",
    "    html = btn_click(wd,url)\n",
    "    result = ([Hall(html) ,league(html), allstar(html), foreigner(html)])\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "df = pd.read_csv('./info/player_my_df.csv')\n",
    "info = pd.read_excel('./info/info2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all the league "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/bs4/__init__.py:179: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4658\n",
      "4758\n",
      "4858\n",
      "4958\n",
      "5058\n"
     ]
    }
   ],
   "source": [
    "chrome_options = Options() \n",
    "chrome_options.add_argument(\"--window-size=200x20\")\n",
    "wd = webdriver.Chrome(options=chrome_options)\n",
    "for i in range(1):\n",
    "    with open ('./all_data3', 'rb') as fp:\n",
    "        all_data = pickle.load(fp)\n",
    "        print(len(all_data))\n",
    "    break\n",
    "\n",
    "url = pd.read_csv('./info/player_my_df.csv')['5']\n",
    "ll = len(url)\n",
    "al = len(all_data)\n",
    "count = 0\n",
    "for i in range(al,ll):\n",
    "    a = get_all_info(wd,url[i])\n",
    "    all_data.append(a)\n",
    "    count += 1\n",
    "    if count == 100:  \n",
    "        print(i)\n",
    "        count = 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5144\n"
     ]
    }
   ],
   "source": [
    "# len(all_data),i\n",
    "# with open('./all_data3', 'wb') as f:\n",
    "#      pickle.dump(all_data, f)\n",
    "# all_data\n",
    "# with open ('./all_data3', 'rb') as fp:\n",
    "#     all_data = pickle.load(fp)\n",
    "#     print(len(all_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_game_table(html):   \n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')    \n",
    "    links = soup.find_all('table', { 'id' : 'per_game'} )\n",
    "    \n",
    "    ## Cannot find the all per game table\n",
    "    if (len(links) == 0):\n",
    "        return\n",
    "\n",
    "    rows = links[0].find_all('tr')\n",
    "\n",
    "    colname = []\n",
    "    rowname = []\n",
    "    for header in links[0].find_all('th', {'scope' : 'col'}):\n",
    "        colname.append(header.text)\n",
    "        \n",
    "    for header in links[0].find_all('th', {'scope' : 'row'}):\n",
    "        rowname.append(header.text)\n",
    "        \n",
    "    table =[]\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    rnum = 0\n",
    "    for row in links[0].find_all('tr'):\n",
    "        table = []\n",
    "        columns = row.find_all('td')\n",
    "        \n",
    "        for column in columns:\n",
    "            ex = column.text.split('-')\n",
    "            try:\n",
    "                if len(ex) > 1 and int(ex[0]):\n",
    "                    rowname.insert(rnum, column.text)\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            table.append(column.text)\n",
    "        \n",
    "        ## The table is NA \n",
    "        if not table :\n",
    "            continue   \n",
    "            \n",
    "        rnum = rnum + 1    \n",
    "        df = df.append(pd.Series(table),ignore_index=True)\n",
    "    \n",
    "    df[colname[0]] = pd.Series(rowname)\n",
    "    df.columns = colname[-len(colname) + 1 :] + colname[:-len(colname) + 1 ]\n",
    "    return(df[colname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the all data processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/bs4/__init__.py:179: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1129\n",
      "1229\n",
      "1329\n",
      "1429\n",
      "1529\n",
      "1629\n",
      "1729\n",
      "1829\n",
      "1929\n",
      "2029\n",
      "2129\n",
      "2229\n",
      "2329\n",
      "2429\n",
      "2529\n",
      "2629\n",
      "2729\n",
      "2829\n",
      "2929\n",
      "3029\n",
      "3129\n",
      "3229\n",
      "3329\n",
      "3429\n",
      "3529\n",
      "3629\n",
      "3729\n",
      "3829\n",
      "3929\n",
      "4029\n",
      "4129\n",
      "4229\n",
      "4329\n",
      "4429\n",
      "4529\n",
      "4629\n",
      "4729\n",
      "4829\n",
      "4929\n",
      "5029\n",
      "5129\n"
     ]
    }
   ],
   "source": [
    "#with open ('./all_data3', 'rb') as fp:\n",
    "#    data = pickle.load(fp)\n",
    "      \n",
    "#info = pd.read_excel('./info/info2.xlsx')\n",
    "\n",
    "col = ['Season', 'Age', 'Tm', 'Lg', 'Pos', 'G', 'GS', 'MP', 'FG', 'FGA', 'FG%',\n",
    "       '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%',\n",
    "       'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS']\n",
    "\n",
    "url = pd.read_csv('./info/player_my_df.csv')['5']\n",
    "#df = pd.DataFrame(columns=col)   \n",
    "row = len(data)\n",
    "ind = info.columns\n",
    "tmp = 0\n",
    "\n",
    "for i in range(1030,row):\n",
    "    \n",
    "    tmp = tmp + 1\n",
    "    if (tmp == 100):\n",
    "        print(i)\n",
    "        tmp = 0\n",
    "    if (tmp == 1000):\n",
    "        clear_output()\n",
    "    \n",
    "    ## Request the web source\n",
    "    response = rq.get(url[i])\n",
    "    html = response.text\n",
    "    per_game_table(html) \n",
    "    my_list = per_game_table(html)\n",
    "    \n",
    "    ## The list may be null or not ##\n",
    "    if my_list is None:\n",
    "        continue\n",
    "    \n",
    "    pp = list(my_list['Season']).index('Career')\n",
    "    my_list = my_list.loc[:pp,:]\n",
    "\n",
    "    small_df = my_list\n",
    "    r , c = ((my_list.shape))\n",
    "    \n",
    "    ## Process empty column from the original table ##\n",
    "    for j in range(len(col)):\n",
    "        if col[j] not in small_df.columns:\n",
    "            small_df[col[j]] = pd.Series([' ']* r)\n",
    "    \n",
    "    ## Process fix award ##\n",
    "    ## [None, ['1988-89'], ['1989-90'], False]\n",
    "    award = data[i]\n",
    "    inf = list(info.iloc[i,:])\n",
    " \n",
    "    ## Hall of Fame ##\n",
    "    if(award[0]):\n",
    "        small_df['名人堂']= pd.Series([1]*r)\n",
    "    else:\n",
    "        small_df['名人堂']= pd.Series([0]*r)\n",
    "\n",
    "    ## Foreigner ##\n",
    "    if(award[3]):\n",
    "        small_df['外籍球員']= pd.Series([1]*r)\n",
    "    else:\n",
    "        small_df['外籍球員']= pd.Series([0]*r)\n",
    "\n",
    "    ## Column initialization ##\n",
    "    small_df['是否入圍全明星'] = pd.Series([0]*r)\n",
    "    small_df['當年度獎項次數'] = pd.Series([0]*r)\n",
    "    small_df['當年度獎項YN'] = pd.Series([0]*r)\n",
    "\n",
    "    ## List all season ##\n",
    "    all_series = list(small_df.iloc[:,0])\n",
    "    for m in range(len(award[2])):\n",
    "        for n in range(len(all_series)):\n",
    "            if(award[2][m] == all_series[n]):\n",
    "                small_df['是否入圍全明星'][n] = 1\n",
    "                small_df['當年度獎項YN'][n] = 1\n",
    "                    \n",
    "                small_df['當年度獎項YN'][r-1] = 1\n",
    "           \n",
    "    for m in range(len(award[1])):\n",
    "        for n in range(len(all_series)):\n",
    "            if(award[1][m] == all_series[n]):\n",
    "                small_df['當年度獎項次數'][n] += 1\n",
    "                small_df['當年度獎項YN'][n] = 1\n",
    "                \n",
    "                ## Career get the award or not\n",
    "                small_df['當年度獎項YN'][r-1] = 1\n",
    "                \n",
    "\n",
    "    ## Count the award throught out all the career ## \n",
    "    ## The player may have the transfer to another \n",
    "     for m in range(r-1):\n",
    "            if m != 0 and all_series[m-1] != all_series[m]:\n",
    "                small_df['當年度獎項次數'][r-1] += small_df['當年度獎項次數'][m]\n",
    "                small_df['是否入圍全明星'][r-1] += small_df['是否入圍全明星'][m]\n",
    "            elif m == 0:\n",
    "                small_df['當年度獎項次數'][r-1] += small_df['當年度獎項次數'][m]\n",
    "                small_df['是否入圍全明星'][r-1] += small_df['是否入圍全明星'][m]\n",
    "                \n",
    "    \n",
    "    ## Process info fix ##\n",
    "    for k in range(len(inf)):\n",
    "        small_df[ind[k]]= pd.Series([inf[k]] * r)\n",
    "   \n",
    "    ## Process series and sort the series ##\n",
    "    year_play = pd.Series([0]*r)  \n",
    "    all_series.pop(-1)    \n",
    "    compact_id = list(set(all_series))\n",
    "    compact_id = sorted(compact_id,key= lambda x : int(x.split('-')[0]))\n",
    "   \n",
    "    ## Process the year player play ##\n",
    "    for j in range(len(all_series)):\n",
    "        front = list(small_df['Tm'])[:j]\n",
    "        count = 0 \n",
    "        \n",
    "        ## The year should minus that he didn't play\n",
    "        for k in range(len(front)):\n",
    "            if 'Did' in front[k]:\n",
    "                count += 1\n",
    "        \n",
    "        year_play[j] = compact_id.index(all_series[j]) - count + 1 \n",
    "    \n",
    "    ### The Career play year is the same as the final ##\n",
    "    year_play[j] = list(year_play)[-2]\n",
    "    small_df['年資'] = year_play\n",
    "    \n",
    "    ## Combine the dataframe ##\n",
    "    df = pd.concat([small_df,df],axis=0,join='outer',ignore_index=True,sort=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "cols = cols[-15:] + cols[:-15]\n",
    "cols = ['FirstName', 'LastName', 'From', 'To',\\\n",
    "        '位置', '呎', '吋', '磅', 'Colleges','出生年', '出生月', \\\n",
    "        '出生日', '選秀順位','名人堂' , '當年度獎項次數', '當年度獎項YN' , '是否入圍全明星','外籍球員', '年資', \\\n",
    "        'Season', 'Age', 'Tm', 'Lg', 'Pos', 'G', 'GS', 'MP',\\\n",
    "        'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%',\\\n",
    "        'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = df[cols]\n",
    "ss.to_excel('GGGGG.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.basketball-reference.com/players/l/lenal01.html'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6835, 50), 924)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33893, 49), (33894, 49))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('FinalVersion.xlsx')\n",
    "dff= pd.read_excel('GGGGG.xlsx')\n",
    "df.shape, dff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['名人堂'] = dff['名人堂']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('FinalVersion.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0        1997-98\n",
       " 1        1998-99\n",
       " 2        1999-00\n",
       " 3        2000-01\n",
       " 4        2001-02\n",
       " 5        2002-03\n",
       " 6        2003-04\n",
       " 7        2004-05\n",
       " 8        2005-06\n",
       " 9        2006-07\n",
       " 10       2007-08\n",
       " 11       2008-09\n",
       " 12       2009-10\n",
       " 13       2010-11\n",
       " 14        Career\n",
       " 15       2003-04\n",
       " 16       2004-05\n",
       " 17       2005-06\n",
       " 18        Career\n",
       " 19       2014-15\n",
       " 20       2014-15\n",
       " 21       2014-15\n",
       " 22        Career\n",
       " 23       2017-18\n",
       " 24        Career\n",
       " 25       2000-01\n",
       " 26       2001-02\n",
       " 27       2002-03\n",
       " 28       2003-04\n",
       " 29       2004-05\n",
       "           ...   \n",
       " 33863    2016-17\n",
       " 33864     Career\n",
       " 33865    2000-01\n",
       " 33866    2001-02\n",
       " 33867    2002-03\n",
       " 33868     Career\n",
       " 33869    1990-91\n",
       " 33870    1991-92\n",
       " 33871     Career\n",
       " 33872    1999-00\n",
       " 33873     Career\n",
       " 33874    1985-86\n",
       " 33875    1986-87\n",
       " 33876    1987-88\n",
       " 33877    1988-89\n",
       " 33878    1989-90\n",
       " 33879    1990-91\n",
       " 33880    1991-92\n",
       " 33881    1992-93\n",
       " 33882    1993-94\n",
       " 33883    1994-95\n",
       " 33884    1995-96\n",
       " 33885    1996-97\n",
       " 33886    1996-97\n",
       " 33887    1996-97\n",
       " 33888    1997-98\n",
       " 33889    1998-99\n",
       " 33890    1999-00\n",
       " 33891    2000-01\n",
       " 33892     Career\n",
       " Name: Season, Length: 33893, dtype: object, 0        1.0\n",
       " 1        0.0\n",
       " 2        0.0\n",
       " 3        0.0\n",
       " 4        0.0\n",
       " 5        0.0\n",
       " 6        0.0\n",
       " 7        0.0\n",
       " 8        0.0\n",
       " 9        0.0\n",
       " 10       0.0\n",
       " 11       0.0\n",
       " 12       0.0\n",
       " 13       0.0\n",
       " 14       0.0\n",
       " 15       0.0\n",
       " 16       0.0\n",
       " 17       0.0\n",
       " 18       0.0\n",
       " 19       0.0\n",
       " 20       0.0\n",
       " 21       0.0\n",
       " 22       0.0\n",
       " 23       0.0\n",
       " 24       0.0\n",
       " 25       0.0\n",
       " 26       0.0\n",
       " 27       0.0\n",
       " 28       0.0\n",
       " 29       0.0\n",
       "         ... \n",
       " 33863    0.0\n",
       " 33864    0.0\n",
       " 33865    0.0\n",
       " 33866    0.0\n",
       " 33867    0.0\n",
       " 33868    0.0\n",
       " 33869    0.0\n",
       " 33870    0.0\n",
       " 33871    0.0\n",
       " 33872    0.0\n",
       " 33873    0.0\n",
       " 33874    0.0\n",
       " 33875    0.0\n",
       " 33876    0.0\n",
       " 33877    1.0\n",
       " 33878    0.0\n",
       " 33879    0.0\n",
       " 33880    0.0\n",
       " 33881    0.0\n",
       " 33882    0.0\n",
       " 33883    0.0\n",
       " 33884    0.0\n",
       " 33885    0.0\n",
       " 33886    0.0\n",
       " 33887    0.0\n",
       " 33888    0.0\n",
       " 33889    0.0\n",
       " 33890    0.0\n",
       " 33891    0.0\n",
       " 33892    0.0\n",
       " Name: 當年度獎項次數, Length: 33893, dtype: float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ss['Season'] , ss['當年度獎項次數'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hall(html):\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('li' , {'class'  : 'bling_special bling_hof'})\n",
    "    for link in links:\n",
    "        if link.text == \"Hall of Fame\":\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.basketball-reference.com/players/r/radzira01.html\"\n",
    "response = rq.get(url) # 用 requests 的 get 方法把網頁抓下來\n",
    "html = response.text # text 屬性就是 html 檔案\n",
    "soup = BeautifulSoup(response.text, \"lxml\") # 指定 lxml 作為解析器\n",
    "# print(soup.prettify()) # 把排版後的 html 印出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hall(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./info/all_data', 'rb') as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.basketball-reference.com/players/s/spitzcr01.html'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Spitzer\n",
       "1        Spitzer\n",
       "2          Smith\n",
       "3          Smith\n",
       "4          Smith\n",
       "5          Smith\n",
       "6          Smith\n",
       "7          Smith\n",
       "8          Smith\n",
       "9        Shelton\n",
       "10       Shelton\n",
       "11       Shelton\n",
       "12       Raymond\n",
       "13       Raymond\n",
       "14       Raymond\n",
       "15       Raymond\n",
       "16       Raymond\n",
       "17       Raymond\n",
       "18       Raymond\n",
       "19       Raymond\n",
       "20       Raymond\n",
       "21       Raymond\n",
       "22          Neal\n",
       "23          Neal\n",
       "24          Neal\n",
       "25          Neal\n",
       "26          Neal\n",
       "27          Neal\n",
       "28        Hodges\n",
       "29        Hodges\n",
       "          ...   \n",
       "6805     Hammons\n",
       "6806     Hammons\n",
       "6807      Guyton\n",
       "6808      Guyton\n",
       "6809      Guyton\n",
       "6810      Guyton\n",
       "6811     English\n",
       "6812     English\n",
       "6813     English\n",
       "6814    Bramlett\n",
       "6815    Bramlett\n",
       "6816       Green\n",
       "6817       Green\n",
       "6818       Green\n",
       "6819       Green\n",
       "6820       Green\n",
       "6821       Green\n",
       "6822       Green\n",
       "6823       Green\n",
       "6824       Green\n",
       "6825       Green\n",
       "6826       Green\n",
       "6827       Green\n",
       "6828       Green\n",
       "6829       Green\n",
       "6830       Green\n",
       "6831       Green\n",
       "6832       Green\n",
       "6833       Green\n",
       "6834       Green\n",
       "Name: LastName, Length: 6835, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.LastName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
