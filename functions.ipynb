{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import requests as rq\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from selenium import webdriver\n",
    "from IPython.display import clear_output\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreigner(html):\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('p')\n",
    "    # print(links)\n",
    "    # <span itemprop=\"birthPlace\">\n",
    "\n",
    "    links = soup.find_all(['span'] , attrs = { 'itemprop' : 'birthPlace'})\n",
    "    for link in links:\n",
    "        for c in (link.descendants):\n",
    "            aa = ((c.find_next('a')).attrs)\n",
    "            if 'country=US' in aa['href']:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should exccute the chromedriver and get the page source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def league(html):\n",
    "    result = []\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('div', attrs = {'class': 'leaderboard_wrapper' , 'id': 'all_leaderboard'})\n",
    "\n",
    "    comments = soup.findAll(text=lambda text:isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        a = comment.find('div_leaderboard')\n",
    "        if (a != -1):\n",
    "            #print(a,comment)\n",
    "            break\n",
    "            \n",
    "    ts = BeautifulSoup(comment.string, 'html.parser')\n",
    "    ts = ts.find_all('div', {'id' : 'leaderboard_all_league' , 'class' : 'data_grid_box'})\n",
    "    for tt in ts:\n",
    "        kk = tt.table.find_all('a')\n",
    "        for k in kk:\n",
    "            result.append((k.text))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should exceute the chromedriver and get the page source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allstar(html):\n",
    "    result = []\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('div', attrs = {'class': 'leaderboard_wrapper' , 'id': 'all_leaderboard'})\n",
    "\n",
    "    comments = soup.findAll(text=lambda text:isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        a = comment.find('div_leaderboard')\n",
    "        if (a != -1):\n",
    "            #print(a,comment)\n",
    "            break\n",
    "    ts = BeautifulSoup(comment.string, 'html.parser')\n",
    "    ts = ts.find_all('div', {'id' : 'leaderboard_allstar' , 'class' : 'data_grid_box'})\n",
    "    for tt in ts:\n",
    "        kk = tt.table.find_all('a')\n",
    "        for k in kk:\n",
    "            result.append((k.text))\n",
    "            \n",
    "    result = list(map (lambda x : str(int(x.split()[0])-1 ) + '-' + (x.split()[0][2:])  , result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hall(html):\n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')\n",
    "    links = soup.find_all('li' , {'class'  : 'bling_special bling_hof'})\n",
    "    for link in links:\n",
    "        if link.text == \"Hall of Fame\":\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def btn_click(wd,url):\n",
    "    wd.get(url)\n",
    "    script = \"vjs_addClass(document.getElementById('leaderboard_allstar'),'show_all')\"\n",
    "    script2 = \"vjs_addClass(document.getElementById('leaderboard_allleague'),'show_all')\"\n",
    "    btn = wd.execute_script(script)\n",
    "    btn = wd.execute_script(script2)\n",
    "    return(wd.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_info(wd,url):\n",
    "    html = btn_click(wd,url)\n",
    "    result = ([Hall(html) ,league(html), allstar(html), foreigner(html)])\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "df = pd.read_csv('./info/player_my_df.csv')\n",
    "info = pd.read_excel('./info/info2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all the league "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options() \n",
    "chrome_options.add_argument(\"--window-size=200x20\")\n",
    "wd = webdriver.Chrome(options=chrome_options)\n",
    "for i in range(1):\n",
    "    with open ('./all_data3', 'rb') as fp:\n",
    "        all_data = pickle.load(fp)\n",
    "        print(len(all_data))\n",
    "    break\n",
    "\n",
    "url = pd.read_csv('./info/player_my_df.csv')['5']\n",
    "ll = len(url)\n",
    "al = len(all_data)\n",
    "count = 0\n",
    "for i in range(al,ll):\n",
    "    a = get_all_info(wd,url[i])\n",
    "    all_data.append(a)\n",
    "    count += 1\n",
    "    if count == 100:  \n",
    "        print(i)\n",
    "        count = 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5144\n"
     ]
    }
   ],
   "source": [
    "len(all_data),i\n",
    "with open('./all_data3', 'wb') as f:\n",
    "     pickle.dump(all_data, f)\n",
    "\n",
    "with open ('./all_data3', 'rb') as fp:\n",
    "    all_data = pickle.load(fp)\n",
    "    print(len(all_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_game_table(html):   \n",
    "    soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')    \n",
    "    links = soup.find_all('table', { 'id' : 'per_game'} )\n",
    "    \n",
    "    ## Cannot find the all per game table\n",
    "    if (len(links) == 0):\n",
    "        return\n",
    "\n",
    "    rows = links[0].find_all('tr')\n",
    "\n",
    "    colname = []\n",
    "    rowname = []\n",
    "    for header in links[0].find_all('th', {'scope' : 'col'}):\n",
    "        colname.append(header.text)\n",
    "        \n",
    "    for header in links[0].find_all('th', {'scope' : 'row'}):\n",
    "        rowname.append(header.text)\n",
    "        \n",
    "    table =[]\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    rnum = 0\n",
    "    for row in links[0].find_all('tr'):\n",
    "        table = []\n",
    "        columns = row.find_all('td')\n",
    "        \n",
    "        for column in columns:\n",
    "            ex = column.text.split('-')\n",
    "            try:\n",
    "                if len(ex) > 1 and int(ex[0]):\n",
    "                    rowname.insert(rnum, column.text)\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            table.append(column.text)\n",
    "        \n",
    "        ## The table is NA \n",
    "        if not table :\n",
    "            continue   \n",
    "            \n",
    "        rnum = rnum + 1    \n",
    "        df = df.append(pd.Series(table),ignore_index=True)\n",
    "    \n",
    "    df[colname[0]] = pd.Series(rowname)\n",
    "    df.columns = colname[-len(colname) + 1 :] + colname[:-len(colname) + 1 ]\n",
    "    return(df[colname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the all data processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open ('./all_data3', 'rb') as fp:\n",
    "#    data = pickle.load(fp)\n",
    "      \n",
    "#info = pd.read_excel('./info/info2.xlsx')\n",
    "\n",
    "col = ['Season', 'Age', 'Tm', 'Lg', 'Pos', 'G', 'GS', 'MP', 'FG', 'FGA', 'FG%',\n",
    "       '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%',\n",
    "       'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS']\n",
    "\n",
    "url = pd.read_csv('./info/player_my_df.csv')['5']\n",
    "#df = pd.DataFrame(columns=col)   \n",
    "row = len(data)\n",
    "ind = info.columns\n",
    "tmp = 0\n",
    "\n",
    "for i in range(1030,row):\n",
    "    \n",
    "    tmp = tmp + 1\n",
    "    if (tmp == 100):\n",
    "        print(i)\n",
    "        tmp = 0\n",
    "    if (tmp == 1000):\n",
    "        clear_output()\n",
    "    \n",
    "    ## Request the web source\n",
    "    response = rq.get(url[i])\n",
    "    html = response.text\n",
    "    per_game_table(html) \n",
    "    my_list = per_game_table(html)\n",
    "    \n",
    "    ## The list may be null or not ##\n",
    "    if my_list is None:\n",
    "        continue\n",
    "    \n",
    "    pp = list(my_list['Season']).index('Career')\n",
    "    my_list = my_list.loc[:pp,:]\n",
    "\n",
    "    small_df = my_list\n",
    "    r , c = ((my_list.shape))\n",
    "    \n",
    "    ## Process empty column from the original table ##\n",
    "    for j in range(len(col)):\n",
    "        if col[j] not in small_df.columns:\n",
    "            small_df[col[j]] = pd.Series([' ']* r)\n",
    "    \n",
    "    ## Process fix award ##\n",
    "    ## [None, ['1988-89'], ['1989-90'], False]\n",
    "    award = data[i]\n",
    "    inf = list(info.iloc[i,:])\n",
    " \n",
    "    ## Hall of Fame ##\n",
    "    if(award[0]):\n",
    "        small_df['名人堂']= pd.Series([1]*r)\n",
    "    else:\n",
    "        small_df['名人堂']= pd.Series([0]*r)\n",
    "\n",
    "    ## Foreigner ##\n",
    "    if(award[3]):\n",
    "        small_df['外籍球員']= pd.Series([1]*r)\n",
    "    else:\n",
    "        small_df['外籍球員']= pd.Series([0]*r)\n",
    "\n",
    "    ## Column initialization ##\n",
    "    small_df['是否入圍全明星'] = pd.Series([0]*r)\n",
    "    small_df['當年度獎項次數'] = pd.Series([0]*r)\n",
    "    small_df['當年度獎項YN'] = pd.Series([0]*r)\n",
    "\n",
    "    ## List all season ##\n",
    "    all_series = list(small_df.iloc[:,0])\n",
    "    for m in range(len(award[2])):\n",
    "        for n in range(len(all_series)):\n",
    "            if(award[2][m] == all_series[n]):\n",
    "                small_df['是否入圍全明星'][n] = 1\n",
    "                small_df['當年度獎項YN'][n] = 1\n",
    "                    \n",
    "                small_df['當年度獎項YN'][r-1] = 1\n",
    "           \n",
    "    for m in range(len(award[1])):\n",
    "        for n in range(len(all_series)):\n",
    "            if(award[1][m] == all_series[n]):\n",
    "                small_df['當年度獎項次數'][n] += 1\n",
    "                small_df['當年度獎項YN'][n] = 1\n",
    "                \n",
    "                ## Career get the award or not\n",
    "                small_df['當年度獎項YN'][r-1] = 1\n",
    "                \n",
    "\n",
    "    ## Count the award throught out all the career ## \n",
    "    ## The player may have the transfer to another \n",
    "     for m in range(r-1):\n",
    "            if m != 0 and all_series[m-1] != all_series[m]:\n",
    "                small_df['當年度獎項次數'][r-1] += small_df['當年度獎項次數'][m]\n",
    "                small_df['是否入圍全明星'][r-1] += small_df['是否入圍全明星'][m]\n",
    "            elif m == 0:\n",
    "                small_df['當年度獎項次數'][r-1] += small_df['當年度獎項次數'][m]\n",
    "                small_df['是否入圍全明星'][r-1] += small_df['是否入圍全明星'][m]\n",
    "                \n",
    "    \n",
    "    ## Process info fix ##\n",
    "    for k in range(len(inf)):\n",
    "        small_df[ind[k]]= pd.Series([inf[k]] * r)\n",
    "   \n",
    "    ## Process series and sort the series ##\n",
    "    year_play = pd.Series([0]*r)  \n",
    "    all_series.pop(-1)    \n",
    "    compact_id = list(set(all_series))\n",
    "    compact_id = sorted(compact_id,key= lambda x : int(x.split('-')[0]))\n",
    "   \n",
    "    ## Process the year player play ##\n",
    "    for j in range(len(all_series)):\n",
    "        front = list(small_df['Tm'])[:j]\n",
    "        count = 0 \n",
    "        \n",
    "        ## The year should minus that he didn't play\n",
    "        for k in range(len(front)):\n",
    "            if 'Did' in front[k]:\n",
    "                count += 1\n",
    "        \n",
    "        year_play[j] = compact_id.index(all_series[j]) - count + 1 \n",
    "    \n",
    "    ### The Career play year is the same as the final ##\n",
    "    year_play[j] = list(year_play)[-2]\n",
    "    small_df['年資'] = year_play\n",
    "    \n",
    "    ## Combine the dataframe ##\n",
    "    df = pd.concat([small_df,df],axis=0,join='outer',ignore_index=True,sort=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "cols = cols[-15:] + cols[:-15]\n",
    "cols = ['FirstName', 'LastName', 'From', 'To',\\\n",
    "        '位置', '呎', '吋', '磅', 'Colleges','出生年', '出生月', \\\n",
    "        '出生日', '選秀順位','名人堂' , '當年度獎項次數', '當年度獎項YN' , '是否入圍全明星','外籍球員', '年資', \\\n",
    "        'Season', 'Age', 'Tm', 'Lg', 'Pos', 'G', 'GS', 'MP',\\\n",
    "        'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%',\\\n",
    "        'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = df[cols]\n",
    "ss.to_excel('GGGGG.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33893, 49), (33894, 49))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('FinalVersion.xlsx')\n",
    "dff= pd.read_excel('GGGGG.xlsx')\n",
    "df.shape, dff.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
